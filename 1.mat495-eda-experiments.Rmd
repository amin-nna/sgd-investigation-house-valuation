---
title: "House Prices Analysis"
author: "Amina Anna Mahamane Ousmane"
date: "2025-04-28"
output:
  pdf_document:
    latex_engine: xelatex
---

```{r setup-and-libraries, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_chunk$set(comment = NA, fig.align = "center")
set.seed(123)
library(tidyverse)
library(readr)
library(GGally) 
library(glmnet)
library(broom)   
library(corrplot)     
library(rlang) 
library(dplyr)
library(ggplot2)
library(ggrepel)
library(ncvreg)
```

## Introduction

This document demonstrates a regression-focused analysis on a high-dimensional house prices dataset (house-prices.csv) that contains 81 columns covering various property, neighborhood, and structural characteristics. Our objective is to predict the final sale price (SalePrice) using these predictors. The analysis will cover:

(a) Exploratory Data Analysis (EDA)
(b) Ordinary Least Squares (OLS) regression
(c) Regularization 
(d) A comparison of the OLS and LASSO models


```{r load-data, message=FALSE}
df_raw <- read.csv("house-prices.csv", stringsAsFactors = FALSE)
```

## (a) Exploratory Data Analysis (EDA)

The dataset contains 1,459 rows and 81 columns, offering a comprehensive view of various aspects of residential properties. It includes basic identifiers (like Id) and structural details such as MSSubClass, MSZoning, and LotFrontage, along with more in-depth features like OverallQual, OverallCond, YearBuilt, and YearRemodAdd. WE also find detailed information on exterior materials, basement finishes, floor areas, and garage attributes, as well as sale-related variables such as SaleType, SaleCondition, and the target variable SalePrice. While many columns provide continuous numerical data (e.g., LotArea, GrLivArea, TotalBsmtSF), there are also several categorical variables (e.g., Neighborhood, HouseStyle, RoofStyle) that capture qualitative aspects of the properties. Overall, this high-dimensional dataset offers a rich mix of variables that can be used to explore and model the factors influencing house prices.
Before modeling, it is important to understand the distribution of each feature, identify missing values, and consider whether any transformations are needed to meet regression assumptions (e.g., normality of residuals).
Additionally, it will be helpful to examine correlations among numerical predictors and the target variable (SalePrice) to see which features appear to be most strongly associated with housing prices. For instance, variables such as square footage of the living area, basement area, or overall quality often correlate highly with sale price. By generating correlation matrices, histograms, and scatterplots, we can gain initial insights into data structure, detect potential outliers, and spot any anomalies that might affect the reliability of our regression models.

We will focus our EDA on the target variable SalePrice and on key predictors that domain knowledge indicate are strongly related to house values.

```{r}
# EDA on Key Numeric Variables
key_numeric_vars <- c("SalePrice", "OverallQual", "GrLivArea", 
                      "TotalBsmtSF", "YearBuilt", "GarageCars", "LotArea")
df_numeric <- df_raw %>% select(any_of(key_numeric_vars))
df_numeric %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Value)) +
  facet_wrap(~ Variable, scales = "free", ncol = 3) +
  geom_histogram(bins = 30, fill = "steelblue", color = "white") +
  labs(title = "Histograms of Key Numeric Variables", x = NULL, y = "Count") +
  theme_minimal()
```

These histograms offer a quick snapshot of the distributions for six key numeric variables:
- **GarageCars:** Most properties have space for 1–2 cars, with a fair number accommodating 3 cars, indicating that 2-car garages are quite common.  
- **GrLivArea:** This is right-skewed, with the majority of houses clustered under 2,000 square feet but a tail extending toward much larger homes.  
- **LotArea:** Strongly right-skewed, as most lots remain under 10,000–12,000 square feet while a few outliers extend to very large plots.  
- **OverallQual:** Shows a somewhat bell-shaped distribution, centered around 5–7, reflecting that most homes are of average or slightly above-average quality.  
- **TotalBsmtSF:** Also right-skewed; many houses have relatively modest basement areas, but some feature extensive basements exceeding 2,000 square feet.  
- **YearBuilt:** Spans well over a century, with noticeable clustering in mid-20th century construction periods and a thinner tail for very old or very new homes.

```{r}
df_numeric %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  ggplot(aes(x = Variable, y = Value)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  coord_flip() +
  labs(title = "Boxplots of Key Numeric Variables", x = "Variable", y = "Value")
```

These boxplots provide a concise overview of the distribution and outliers for each numeric variable:
- **LotArea** stands out with a large number of outliers, reflecting the broad range of lot sizes—from small plots to very expansive properties.  
- **OverallQual** is an ordinal variable (1–10), which naturally restricts its spread. Most values cluster around the middle range (around 5–7), indicating average to slightly above-average quality.  
- **GrLivArea** and **TotalBsmtSF** show a right-skewed pattern, with a moderate interquartile range but a tail of larger homes featuring significantly more living or basement area.  
- **YearBuilt** spans from the late 1800s through the 2000s, showing a fairly wide range but few extreme outliers—most properties fall within a century-long window.  
- **GarageCars** is quite compact, with most homes accommodating one to two cars, and relatively few properties providing space for three or more vehicles.

```{r}
corr_matrix_key <- cor(df_numeric, use = "complete.obs")
corrplot(corr_matrix_key, 
         method = "circle",    # alternatives: "color", "number"
         type = "upper",       # display only the upper triangle
         tl.cex = 0.8,         # text label size
         title = "Correlation Plot of Key Numeric Variables", 
         mar = c(0, 0, 1, 0))
```
This correlation matrix reveals how strongly each numeric variable is linearly related to the others:
- **OverallQual** shows moderate-to-strong positive correlations with measures of house size and capacity (e.g., `GrLivArea`, `TotalBsmtSF`, and `GarageCars`). This suggests that higher-quality homes also tend to have more living space, more basement area, and larger garages.  
- **GrLivArea** and **TotalBsmtSF** are fairly strongly correlated, indicating that houses with large above-ground living areas often also have sizeable basements.  
- **YearBuilt** is positively correlated with the other variables but to a lesser degree. This implies that newer homes might be slightly bigger or higher quality, though the effect is not as pronounced as size-related variables.  
- **GarageCars** tracks moderately with both `GrLivArea` and `OverallQual`, suggesting that homes with more garage space often have greater overall quality and living area.  
- **LotArea** exhibits relatively low correlations with most other variables, implying that a larger lot size doesn’t necessarily coincide with higher quality, bigger living area, or newer construction.
Overall, these correlations highlight which features tend to move together (e.g., bigger homes are often higher quality and have more garage space) and which features vary more independently (e.g., lot size).

```{r}
ggpairs(df_numeric)
```

This **scatterplot matrix** (via `GGally::ggpairs`) gives a side-by-side look at both the distributions of each variable (on the diagonal) and their pairwise relationships (in the off-diagonal plots). Here are some key observations:
**Diagonal (Univariate Distributions):**  
   - **OverallQual**: A somewhat bell-shaped distribution with most ratings between 5 and 7.  
   - **GrLivArea** and **TotalBsmtSF**: Right-skewed distributions, indicating that most houses have moderate living and basement areas, but a few are significantly larger.  
   - **YearBuilt**: Spans a broad range, though most observations cluster around mid-20th century construction.  
   - **GarageCars**: Discrete distribution (integer values). Most homes have space for 1–2 cars.  
   - **LotArea**: Highly right-skewed, with a small number of very large lots.

**Off-Diagonal (Pairwise Scatterplots & Correlations):**  
   - **OverallQual** vs. SalePrice: Strong positive correlation, confirming that higher-quality homes tend to sell for more.  
   - **GrLivArea** vs. SalePrice: Also a notable positive correlation—larger living areas generally command higher prices.  
   - **TotalBsmtSF** vs. SalePrice: A moderate-to-strong positive relationship, suggesting bigger basements can add value.  
   - **YearBuilt** vs. SalePrice: Moderate correlation. While newer homes often sell for more, other factors (size, quality) appear to be more influential.  
   - **GarageCars** vs. SalePrice: Positive correlation, but less pronounced than living area or overall quality.  
   - **LotArea** vs. SalePrice: Some positive correlation, yet weaker than for interior features, indicating that sheer lot size alone doesn’t always dictate higher sale prices.

Overall, the scatterplot matrix confirms that **home size (living area, basement size), quality, and garage capacity** play major roles in determining sale price, while **lot size and year built** appear somewhat less impactful but still relevant.

```{r}
# EDA on Key Categorical Variables
key_cat_vars <- c("Neighborhood", "HouseStyle")
df_cat <- df_raw %>% select(any_of(key_cat_vars))

# Bar plots for each categorical variable
for (var in names(df_cat)) {
  p <- ggplot(df_cat, aes(x = !!sym(var))) +
    geom_bar(fill = "skyblue", color = "black") +
    labs(title = paste("Frequency of", var), x = var, y = "Count") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  print(p)
}
```
The bar plot of Neighborhood shows that the most common neighborhoods in our dataset are NAmes, CollgCr, and OldTown, each contributing a substantial number of observations. Conversely, some neighborhoods (like Blueste and Veenker) have far fewer properties, indicating potential data sparsity in those areas. Meanwhile, the HouseStyle bar plot reveals that 1Story homes are by far the most prevalent, followed by 2Story homes, with relatively few properties categorized as 1.5Fin, 1.5Unf, or 2.5Unf. These distributions highlight which categories dominate our data. For instance, sparse categories might have less predictive power.

## (b) Ordinary Least Squares (OLS) regression

```{r}
# Impute missing numeric values with the median
numeric_cols <- names(df_raw)[sapply(df_raw, is.numeric)]
for (col in numeric_cols) {
  df_raw[[col]][is.na(df_raw[[col]])] <- median(df_raw[[col]], na.rm = TRUE)
}

# Impute missing categorical values with "None"
categorical_cols <- names(df_raw)[sapply(df_raw, is.character)]
for (col in categorical_cols) {
  df_raw[[col]][is.na(df_raw[[col]])] <- "None"
}

# Convert character columns to factors
df_raw <- df_raw %>% mutate_if(is.character, as.factor)

# Refresh factor levels
df_clean <- df_raw %>% mutate_if(is.factor, droplevels)

# Identify and remove factor columns with fewer than 2 levels
bad_factors <- sapply(df_clean, function(x) is.factor(x) && length(levels(x)) < 2)
if (any(bad_factors)) {
  cat("Removing constant factor variables:\n")
  print(names(bad_factors)[bad_factors])
  df_clean <- df_clean %>% select(-one_of(names(bad_factors)[bad_factors]))
}

# Verify the number of rows and structure of the cleaned data
cat("Number of observations:", nrow(df_clean), "\n")

# Fit the OLS regression model to predict SalePrice using all predictors
model <- lm(SalePrice ~ ., data = df_clean)

# Display the summary of the model
summary(model)
```
From the summary, we see the model attempts to estimate 254 parameters in total:  
- **1 intercept**      
- **253 predictors** (as indicated by “on 253 and 1206 DF” in the F-statistic).    

However, it also reports “(8 not defined because of singularities).” That means 8 parameters could not be estimated (likely due to perfect collinearity). Thus, there are 254 coefficient “slots” in the design, but only 246 can actually be fit in the final model.    

- The model explains approximately 93% of the variance in SalePrice (\(R^2 \approx 0.9333\)) and has an adjusted \(R^2 \approx 0.9193\).    
- The overall \(F\)-statistic (66.68, \(p < 2.2 \times 10^{-16}\)) confirms that at least some predictors significantly affect SalePrice.    
- The residual standard error is roughly \$22,570, indicating the typical deviation of predictions from actual sale prices.   

Several kitchen-quality indicators (e.g., KitchenQualGd, KitchenQualTA) and the number of bedrooms (BedroomAbvGr) show significant negative coefficients. This can happen when other correlated predictors (such as square footage) capture the “positive” effect, causing some intuitive variables to appear negatively associated.  

```{r}
# Plot diagnostic plots for the model
par(mfrow = c(2, 2))
plot(model)
```
- Residuals vs. Fitted: Suggests a reasonably good fit, though minor patterns may indicate slight non-linearity or heteroskedasticity.  
- Q–Q Plot: Residuals largely follow a straight line, but slight deviations at the tails suggest mild departures from normality.  
- Scale-Location Plot: Shows a possible increase in residual spread at higher fitted values, hinting at heteroskedasticity.  
- Residuals vs. Leverage: Identifies observations with leverage = 1 (very influential points). These are homes with unique or extreme feature combinations. Investigating or removing them could change the model fit.  

Diagnostic plots revealed several observations with high leverage (leverage values approaching 1), indicating that these influential points may disproportionately affect the model estimates. To address this, I examined these data points in detail to determine if they represent data entry errors or genuine outliers. Additionally, the Scale-Location and Q–Q plots suggested some heteroskedasticity and mild departures from normality. As a result, I plan to explore transformations, such as applying a logarithmic transformation to the response variable (SalePrice), to stabilize variance and improve the distribution of residuals. These steps will help refine the model and ensure more robust and interpretable results.

```{r}
n <- nrow(df_clean)   # number of observations
p <- length(coef(model)) - 1   # number of predictors (excluding intercept)
# Calculate Cook's distance
cooks_d <- cooks.distance(model)
# Set a common threshold for Cook's distance: 4/(n-p-1)
cooks_threshold <- 4 / (n - p - 1)

# Create a data frame for plotting Cook's distance
df_cooks <- data.frame(
  index = seq_along(cooks_d),
  cooks_dist = cooks_d
)

# Plot Cook's distance using ggplot2
ggplot(df_cooks, aes(x = index, y = cooks_dist)) +
  geom_segment(aes(xend = index, yend = 0), color = "gray70") +
  geom_point(color = "black") +
  geom_hline(yintercept = cooks_threshold, color = "red", linetype = "dashed") +
  # Label only points above the threshold
  geom_text_repel(
    data = subset(df_cooks, cooks_dist > cooks_threshold),
    aes(label = index),
    color = "red",
    size = 3,
    max.overlaps = 50
  ) +
  theme_minimal() +
  labs(
    title = "Cook's Distance",
    x = "Observation Index",
    y = "Cook's Distance"
  )
```
Cook’s Distance measures how much a single observation affects the overall regression model. Most points have relatively low influence, but a few observations—such as 1171 and 1424—exhibit exceptionally high Cook’s Distance values, indicating that they significantly impact the regression coefficients. These influential points warrant further investigation to determine whether they represent true outliers, data entry errors, or cases where model adjustments (such as robust regression or transformations) may be necessary to mitigate their effect.

```{r}
# Fit a model with a log transformation of SalePrice
model_log <- lm(log(SalePrice) ~ ., data = df_clean)
summary(model_log)
```
The log-transformed regression model, with \(\log(\text{SalePrice})\) as the response variable, shows an excellent fit with a Multiple R-squared of 0.9441 and an Adjusted R-squared of 0.9323, indicating that approximately 94% of the variation in the log sale price is explained by the predictors. The overall model is highly significant (F-statistic = 80.43, \(p < 2.2 \times 10^{-16}\)), and the residual standard error is relatively low (0.1039), suggesting a precise fit. Notably, several predictors, such as the different levels of MSZoning and LotArea, are statistically significant and contribute meaningfully to explaining sale price variability. The log transformation appears to have stabilized variance and improved normality in the residuals, addressing potential issues of heteroskedasticity and skewness that were observed in the original scale. Some predictors were omitted due to singularities, but overall the model provides a robust framework for understanding the determinants of sale price on the log scale.
The model is linear in its formulation—it assumes that a linear combination of the predictors explains the (transformed) response. In our case, we modeled log (SalePrice) log(SalePrice) as a linear function of the predictors, so the relationship is linear on the log scale. This means that changes in the predictors are assumed to have a constant proportional effect on SalePrice. 

```{r}
# Plot diagnostic plots for the log-transformed model (base R)
par(mfrow = c(2, 2))
plot(model_log)
```
The Residuals vs. Fitted plot (top-left) shows how residuals deviate from zero across different fitted values; ideally, the red smoothing line should be flat, indicating no systematic pattern. The Q–Q Residuals plot (top-right) checks the normality of residuals; here, points closely follow the diagonal except for mild deviations in the tails. The Scale-Location plot (bottom-left) helps evaluate homoscedasticity (constant variance); the near-horizontal red line suggests no severe heteroskedasticity. Finally, the Residuals vs. Leverage plot (bottom-right) identifies potentially influential observations—such as #8260—whose leverage or Cook’s distance values are high.


## (c) Regularization 

```{r}
set.seed(123)
X_mat <- model.matrix(SalePrice ~ . - 1, data = df_clean)
y_vec <- log(df_clean$SalePrice)

# Number of observations and total sum of squares for y
n <- length(y_vec)
SST <- sum((y_vec - mean(y_vec))^2)

# ---------------------------
# 1. SCAD Regularization 
# ---------------------------

# Fit the full solution path using ncvreg()
scad_fit <- ncvreg(X_mat, y_vec, penalty = "SCAD")
# Perform cross-validation over the full solution path
cv_scad <- cv.ncvreg(X_mat, y_vec, penalty = "SCAD")
best_lambda_scad <- cv_scad$lambda.min
# Extract coefficients at the best lambda from the full path
scad_coefs <- coef(scad_fit, lambda = best_lambda_scad)
# Compute predictions and performance metrics
scad_pred <- predict(scad_fit, X_mat, lambda = best_lambda_scad)
scad_mse <- mean((y_vec - scad_pred)^2)
scad_R2 <- 1 - sum((y_vec - scad_pred)^2) / SST
scad_nonzero <- sum(scad_coefs != 0) - 1  # subtract intercept
scad_adj_R2 <- 1 - (1 - scad_R2) * (n - 1) / (n - scad_nonzero - 1)

# ---------------------------
# 2. glmnet Regularization
# ---------------------------

# Create a design matrix for glmnet (automatically one-hot encodes factors)
X <- model.matrix(SalePrice ~ ., data = df_clean)[, -1]
y <- log(df_clean$SalePrice)

# --- LASSO ---
cv_lasso <- cv.glmnet(X, y, alpha = 1)
best_lambda_lasso <- cv_lasso$lambda.min
lasso_model <- glmnet(X, y, alpha = 1, lambda = best_lambda_lasso)
lasso_pred <- predict(lasso_model, X)
lasso_mse <- mean((y - lasso_pred)^2)
nonzero_lasso <- sum(coef(lasso_model) != 0) - 1  # subtract intercept
lasso_R2 <- 1 - sum((y - lasso_pred)^2) / sum((y - mean(y))^2)
lasso_adj_R2 <- 1 - (1 - lasso_R2) * (n - 1) / (n - nonzero_lasso - 1)

# --- Ridge ---
cv_ridge <- cv.glmnet(X, y, alpha = 0)
best_lambda_ridge <- cv_ridge$lambda.min
ridge_model <- glmnet(X, y, alpha = 0, lambda = best_lambda_ridge)
ridge_pred <- predict(ridge_model, X)
ridge_mse <- mean((y - ridge_pred)^2)
nonzero_ridge <- sum(coef(ridge_model) != 0) - 1
ridge_R2 <- 1 - sum((y - ridge_pred)^2) / sum((y - mean(y))^2)
ridge_adj_R2 <- 1 - (1 - ridge_R2) * (n - 1) / (n - nonzero_ridge - 1)

# --- Elastic Net (with different alphas) ---
alphas <- c(0.25, 0.5, 0.75)
mse_vec <- numeric(length(alphas))
nonzero_vec <- numeric(length(alphas))
adjR2_vec <- numeric(length(alphas))
enet_models <- list()

for(i in seq_along(alphas)){
  cv_enet <- cv.glmnet(X, y, alpha = alphas[i])
  best_lambda_enet <- cv_enet$lambda.min
  enet_model <- glmnet(X, y, alpha = alphas[i], lambda = best_lambda_enet)
  enet_models[[i]] <- enet_model
  pred_enet <- predict(enet_model, X)
  mse_vec[i] <- mean((y - pred_enet)^2)
  nonzero_vec[i] <- sum(coef(enet_model) != 0) - 1
  R2_enet <- 1 - sum((y - pred_enet)^2) / sum((y - mean(y))^2)
  adjR2_vec[i] <- 1 - (1 - R2_enet) * (n - 1) / (n - nonzero_vec[i] - 1)
}

# ---------------------------
# 3. Summarize Regularization Results
# ---------------------------
results <- data.frame(
  Method = c("SCAD", "LASSO", "Ridge", 
             paste0("Elastic Net (alpha=", alphas, ")")),
  MSE = c(scad_mse, lasso_mse, ridge_mse, mse_vec),
  Nonzero_Coeffs = c(scad_nonzero, nonzero_lasso, nonzero_ridge, nonzero_vec),
  Adjusted_R2 = c(scad_adj_R2, lasso_adj_R2, ridge_adj_R2, adjR2_vec)
)
print(results)
```
Based on repeated runs of the regularization code, the SCAD offer best MSE & Adjusted R² with fewest variables, it balances predictive performance and simplicity, which is ideal for interpretability in a high-dimensional setting.

```{r}
# ---------------------------------------
# Timing OLS model fitting on log(SalePrice)
# ---------------------------------------
ols_time <- system.time({
  model_ols_timed <- lm(log(SalePrice) ~ ., data = df_clean)
})

# ---------------------------------------
# Timing SCAD model fitting on log(SalePrice)
# ---------------------------------------
scad_time <- system.time({
  scad_fit_timed <- ncvreg(X_mat, y_vec, penalty = "SCAD")
  cv_scad_timed <- cv.ncvreg(X_mat, y_vec, penalty = "SCAD")
})

# ---------------------------------------
# Timing LASSO model fitting on log(SalePrice)
# ---------------------------------------
lasso_time <- system.time({
  cv_lasso_timed <- cv.glmnet(X, y_vec, alpha = 1)
  best_lambda_lasso_timed <- cv_lasso_timed$lambda.min
  lasso_model_timed <- glmnet(X, y_vec, alpha = 1, lambda = best_lambda_lasso_timed)
})

# ---------------------------------------
# Timing Ridge model fitting on log(SalePrice)
# ---------------------------------------
ridge_time <- system.time({
  cv_ridge_timed <- cv.glmnet(X, y_vec, alpha = 0)
  best_lambda_ridge_timed <- cv_ridge_timed$lambda.min
  ridge_model_timed <- glmnet(X, y_vec, alpha = 0, lambda = best_lambda_ridge_timed)
})

# ---------------------------------------
# Timing Elastic Net model fitting (alpha = 0.5) on log(SalePrice)
# ---------------------------------------
enet_time <- system.time({
  cv_enet_timed <- cv.glmnet(X, y_vec, alpha = 0.5)
  best_lambda_enet_timed <- cv_enet_timed$lambda.min
  enet_model_timed <- glmnet(X, y_vec, alpha = 0.5, lambda = best_lambda_enet_timed)
})

```

```{r}
# Create a dataframe summarizing training time
timing_results_log <- data.frame(
  Model = c("OLS (log)", "SCAD (log)", "LASSO (log)", "Ridge (log)", "Elastic Net (log)"),
  Time_Seconds = c(
    ols_time["elapsed"],
    scad_time["elapsed"],
    lasso_time["elapsed"],
    ridge_time["elapsed"],
    enet_time["elapsed"]
  )
)

# Print timing table
print(timing_results_log)

# Plot the computation time
ggplot(timing_results_log, aes(x = Model, y = Time_Seconds, fill = Model)) +
  geom_bar(stat = "identity") +
  labs(title = "Comparison of Model Training Times (log-centered SalePrice)",
       x = "Model",
       y = "Time (seconds)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

As shown in the bar chart, the SCAD model—while offering strong performance in terms of feature selection—was by far the most computationally expensive, taking over 19 seconds to train on the log-transformed sale price. In contrast, OLS (log) was the fastest at 0.14 seconds, confirming its efficiency even in high-dimensional settings. LASSO and Elastic Net both took around 1–1.2 seconds, reflecting the extra cost of regularization and cross-validation. Ridge, slightly faster at 0.66 seconds, strikes a good balance. Overall, this highlights that SCAD's power comes with a significant time cost, whereas OLS and Ridge are much quicker, and LASSO/Elastic Net offer a good compromise between sparsity and speed.

```{r}
# Convert the SCAD coefficient matrix to a regular matrix
scad_mat <- as.matrix(scad_coefs)

# Remove the intercept row (assuming it’s labeled as "(Intercept)")
scad_mat <- scad_mat[rownames(scad_mat) != "(Intercept)", , drop = FALSE]

# Create a data frame with predictor names and their coefficient values
scad_df <- data.frame(Predictor = rownames(scad_mat), 
                      Coefficient = scad_mat[, 1], 
                      row.names = NULL)

# Filter to only include nonzero coefficients
nonzero_scad_df <- scad_df[scad_df$Coefficient != 0, ]

cat("Predictors selected by SCAD:\n")
print(nonzero_scad_df, row.names = FALSE)

```


## (d) A comparison of the OLS and regularized models
Based on the model comparisons performed on the log-transformed SalePrice, the Ordinary Least Squares (OLS) regression achieved the best overall performance, with the lowest Mean Squared Error (MSE \( \approx 0.0108 \)) and the highest Adjusted \( R^2 \) (0.9323). It was also the fastest model to train (0.14 seconds), demonstrating excellent computational efficiency. However, OLS does not induce sparsity, utilizing nearly all predictors, which could complicate model interpretability.
Among the regularized models, SCAD achieved strong sparsity (only 64 nonzero coefficients) and relatively good predictive accuracy (MSE \( \approx 0.0142 \)), but it was significantly slower to train (19.17 seconds). LASSO and Elastic Net (\( \alpha = 0.5 \)) offered balanced alternatives, maintaining moderate sparsity (83–96 predictors), good Adjusted \( R^2 \) (around 0.90), and faster training times (~1 second).
Overall, if computational time and pure predictive accuracy are prioritized, **OLS remains the most efficient and accurate model** for this high-dimensional house price dataset. However, **if interpretability and model simplicity are privileged, SCAD or LASSO represent better choices**, offering a substantial reduction in the number of predictors while maintaining strong predictive performance.